{
  "attention_class": {
    "class_name": "Attention",
    "module": "timm.models.vision_transformer",
    "attributes": {
      "T_destination": "TypeVar",
      "_apply": "method, signature=(fn, recurse=True)",
      "_backward_hooks": "OrderedDict",
      "_backward_pre_hooks": "OrderedDict",
      "_buffers": "OrderedDict",
      "_call_impl": "method, signature=(*args, **kwargs)",
      "_compiled_call_impl": "NoneType",
      "_forward_hooks": "OrderedDict",
      "_forward_hooks_always_called": "OrderedDict",
      "_forward_hooks_with_kwargs": "OrderedDict",
      "_forward_pre_hooks": "OrderedDict",
      "_forward_pre_hooks_with_kwargs": "OrderedDict",
      "_get_backward_hooks": "method, signature=()",
      "_get_backward_pre_hooks": "method, signature=()",
      "_get_name": "method, signature=()",
      "_is_full_backward_hook": "NoneType",
      "_is_hf_initialized": "bool",
      "_load_from_state_dict": "method, signature=(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)",
      "_load_state_dict_post_hooks": "OrderedDict",
      "_load_state_dict_pre_hooks": "OrderedDict",
      "_maybe_warn_non_full_backward_hook": "method, signature=(inputs, result, grad_fn)",
      "_modules": "OrderedDict",
      "_named_members": "method, signature=(get_members_fn, prefix='', recurse=True, remove_duplicate: bool = True)",
      "_non_persistent_buffers_set": "set",
      "_parameters": "OrderedDict",
      "_register_load_state_dict_pre_hook": "method, signature=(hook, with_module=False)",
      "_register_state_dict_hook": "method, signature=(hook)",
      "_replicate_for_data_parallel": "method, signature=()",
      "_save_to_state_dict": "method, signature=(destination, prefix, keep_vars)",
      "_slow_forward": "method, signature=(*input, **kwargs)",
      "_state_dict_hooks": "OrderedDict",
      "_state_dict_pre_hooks": "OrderedDict",
      "_version": "int",
      "_wrapped_call_impl": "method, signature=(*args, **kwargs)",
      "add_module": "method, signature=(name: str, module: Optional[ForwardRef('Module')]) -> None",
      "apply": "method, signature=(fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T",
      "attn_drop": "Dropout, signature=(*args, **kwargs)",
      "bfloat16": "method, signature=() -> ~T",
      "buffers": "method, signature=(recurse: bool = True) -> Iterator[torch.Tensor]",
      "call_super_init": "bool",
      "children": "method, signature=() -> Iterator[ForwardRef('Module')]",
      "compile": "method, signature=(*args, **kwargs)",
      "cpu": "method, signature=() -> ~T",
      "cuda": "method, signature=(device: Union[int, torch.device, NoneType] = None) -> ~T",
      "double": "method, signature=() -> ~T",
      "dump_patches": "bool",
      "eval": "method, signature=() -> ~T",
      "extra_repr": "method, signature=() -> str",
      "float": "method, signature=() -> ~T",
      "forward": "method, signature=(x)",
      "fused_attn": "bool",
      "get_buffer": "method, signature=(target: str) -> 'Tensor'",
      "get_extra_state": "method, signature=() -> Any",
      "get_parameter": "method, signature=(target: str) -> 'Parameter'",
      "get_submodule": "method, signature=(target: str) -> 'Module'",
      "half": "method, signature=() -> ~T",
      "head_dim": "int",
      "ipu": "method, signature=(device: Union[int, torch.device, NoneType] = None) -> ~T",
      "k_norm": "Identity, signature=(*args, **kwargs)",
      "load_state_dict": "method, signature=(state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False)",
      "modules": "method, signature=() -> Iterator[ForwardRef('Module')]",
      "named_buffers": "method, signature=(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.Tensor]]",
      "named_children": "method, signature=() -> Iterator[Tuple[str, ForwardRef('Module')]]",
      "named_modules": "method, signature=(memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)",
      "named_parameters": "method, signature=(prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]",
      "num_heads": "int",
      "parameters": "method, signature=(recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]",
      "proj": "Linear, signature=(*args, **kwargs)",
      "proj_drop": "Dropout, signature=(*args, **kwargs)",
      "q_norm": "Identity, signature=(*args, **kwargs)",
      "qkv": "Linear, signature=(*args, **kwargs)",
      "register_backward_hook": "method, signature=(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle",
      "register_buffer": "method, signature=(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None",
      "register_forward_hook": "method, signature=(hook: Union[Callable[[~T, Tuple[Any, ...], Any], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle",
      "register_forward_pre_hook": "method, signature=(hook: Union[Callable[[~T, Tuple[Any, ...]], Optional[Any]], Callable[[~T, Tuple[Any, ...], Dict[str, Any]], Optional[Tuple[Any, Dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle",
      "register_full_backward_hook": "method, signature=(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
      "register_full_backward_pre_hook": "method, signature=(hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, Tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle",
      "register_load_state_dict_post_hook": "method, signature=(hook)",
      "register_module": "method, signature=(name: str, module: Optional[ForwardRef('Module')]) -> None",
      "register_parameter": "method, signature=(name: str, param: Optional[torch.nn.parameter.Parameter]) -> None",
      "register_state_dict_pre_hook": "method, signature=(hook)",
      "requires_grad_": "method, signature=(requires_grad: bool = True) -> ~T",
      "scale": "float",
      "set_extra_state": "method, signature=(state: Any)",
      "share_memory": "method, signature=() -> ~T",
      "state_dict": "method, signature=(*args, destination=None, prefix='', keep_vars=False)",
      "to": "method, signature=(*args, **kwargs)",
      "to_empty": "method, signature=(*, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> ~T",
      "train": "method, signature=(mode: bool = True) -> ~T",
      "training": "bool",
      "type": "method, signature=(dst_type: Union[torch.dtype, str]) -> ~T",
      "xpu": "method, signature=(device: Union[int, torch.device, NoneType] = None) -> ~T",
      "zero_grad": "method, signature=(set_to_none: bool = True) -> None"
    }
  },
  "qkv_computation": {
    "qkv_related": [
      "_backward_hooks",
      "_backward_pre_hooks",
      "_forward_hooks",
      "_forward_hooks_always_called",
      "_forward_hooks_with_kwargs",
      "_forward_pre_hooks",
      "_forward_pre_hooks_with_kwargs",
      "_get_backward_hooks",
      "_get_backward_pre_hooks",
      "_is_full_backward_hook",
      "_load_state_dict_post_hooks",
      "_load_state_dict_pre_hooks",
      "_maybe_warn_non_full_backward_hook",
      "_register_load_state_dict_pre_hook",
      "_register_state_dict_hook",
      "_save_to_state_dict",
      "_state_dict_hooks",
      "_state_dict_pre_hooks",
      "_version",
      "eval",
      "k_norm",
      "q_norm",
      "qkv",
      "register_backward_hook",
      "register_forward_hook",
      "register_forward_pre_hook",
      "register_full_backward_hook",
      "register_full_backward_pre_hook",
      "register_load_state_dict_post_hook",
      "register_state_dict_pre_hook",
      "requires_grad_"
    ],
    "hooked_methods": [
      "forward"
    ]
  },
  "attention_flow": {
    "vision_backbone_type": "PrismaticVisionBackbone",
    "projector_type": "PrismaticProjector",
    "projector_layers": [
      {
        "name": "fc1",
        "type": "Linear",
        "shape": "torch.Size([8704, 2176])"
      },
      {
        "name": "fc2",
        "type": "Linear",
        "shape": "torch.Size([4096, 8704])"
      },
      {
        "name": "fc3",
        "type": "Linear",
        "shape": "torch.Size([4096, 4096])"
      },
      {
        "name": "act_fn1",
        "type": "GELU",
        "shape": "unknown"
      },
      {
        "name": "act_fn2",
        "type": "GELU",
        "shape": "unknown"
      }
    ],
    "language_model_type": "LlamaForCausalLM",
    "processor_type": "PrismaticProcessor",
    "processor_attributes": [
      "_auto_class: str",
      "_create_repo(): method",
      "_get_arguments_from_pretrained(): method",
      "_get_files_timestamps(): method",
      "_upload_modified_files(): method",
      "attributes: list",
      "batch_decode(): method",
      "decode(): method",
      "feature_extractor_class: NoneType",
      "from_args_and_dict(): method",
      "from_pretrained(): method",
      "get_processor_dict(): method",
      "image_processor(): PrismaticImageProcessor",
      "image_processor_class: str",
      "model_input_names: list",
      "push_to_hub(): method",
      "register_for_auto_class(): method",
      "save_pretrained(): method",
      "to_dict(): method",
      "to_json_file(): method",
      "to_json_string(): method",
      "tokenizer(): LlamaTokenizerFast",
      "tokenizer_class: str"
    ],
    "step_method_code": "    def step(\n        self, image: np.ndarray, task_description: Optional[str] = None, *args, **kwargs\n    ) -> tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:\n        \"\"\"\n        Input:\n            image: np.ndarray of shape (H, W, 3), uint8\n            task_description: Optional[str], task description; if different from previous task description, policy state is reset\n        Output:\n            raw_action: dict; raw policy action output\n            action: dict; processed action to be sent to the maniskill2 environment, with the following keys:\n                - 'world_vector': np.ndarray of shape (3,), xyz translation of robot end-effector\n                - 'rot_axangle': np.ndarray of shape (3,), axis-angle representation of end-effector rotation\n                - 'gripper': np.ndarray of shape (1,), gripper action\n                - 'terminate_episode': np.ndarray of shape (1,), 1 if episode should be terminated, 0 otherwise\n        \"\"\"\n        if task_description is not None:\n            if task_description != self.task_description:\n                self.reset(task_description)\n\n        assert image.dtype == np.uint8\n        image = self._resize_image(image)\n\n        image: Image.Image = Image.fromarray(image)\n        prompt = task_description\n\n        # predict action (7-dof; un-normalize for bridgev2)\n        inputs = self.processor(prompt, image).to(\"cuda:0\", dtype=torch.bfloat16)\n        raw_actions = self.vla.predict_action(**inputs, unnorm_key=self.unnorm_key, do_sample=False)[None]\n        # print(f\"*** raw actions {raw_actions} ***\")\n\n        raw_action = {\n            \"world_vector\": np.array(raw_actions[0, :3]),\n            \"rotation_delta\": np.array(raw_actions[0, 3:6]),\n            \"open_gripper\": np.array(raw_actions[0, 6:7]),  # range [0, 1]; 1 = open; 0 = close\n        }\n\n        # process raw_action to obtain the action to be sent to the maniskill2 environment\n        action = {}\n        action[\"world_vector\"] = raw_action[\"world_vector\"] * self.action_scale\n        action_rotation_delta = np.asarray(raw_action[\"rotation_delta\"], dtype=np.float64)\n        roll, pitch, yaw = action_rotation_delta\n        action_rotation_ax, action_rotation_angle = euler2axangle(roll, pitch, yaw)\n        action_rotation_axangle = action_rotation_ax * action_rotation_angle\n        action[\"rot_axangle\"] = action_rotation_axangle * self.action_scale\n\n        if self.policy_setup == \"google_robot\":\n            current_gripper_action = raw_action[\"open_gripper\"]\n            if self.previous_gripper_action is None:\n                relative_gripper_action = np.array([0])\n            else:\n                relative_gripper_action = self.previous_gripper_action - current_gripper_action\n            self.previous_gripper_action = current_gripper_action\n\n            if np.abs(relative_gripper_action) > 0.5 and (not self.sticky_action_is_on):\n                self.sticky_action_is_on = True\n                self.sticky_gripper_action = relative_gripper_action\n\n            if self.sticky_action_is_on:\n                self.gripper_action_repeat += 1\n                relative_gripper_action = self.sticky_gripper_action\n\n            if self.gripper_action_repeat == self.sticky_gripper_num_repeat:\n                self.sticky_action_is_on = False\n                self.gripper_action_repeat = 0\n                self.sticky_gripper_action = 0.0\n\n            action[\"gripper\"] = relative_gripper_action\n\n        elif self.policy_setup == \"widowx_bridge\":\n            action[\"gripper\"] = 2.0 * (raw_action[\"open_gripper\"] > 0.5) - 1.0\n\n        action[\"terminate_episode\"] = np.array([0.0])\n\n        return raw_action, action\n",
    "predict_action_code": "    def predict_action(\n        self, input_ids: Optional[torch.LongTensor] = None, unnorm_key: Optional[str] = None, **kwargs\n    ) -> np.ndarray:\n        \"\"\"Thin wrapper around .generate() that decodes predicted actions and unnormalizes them.\"\"\"\n        # We need to add this special empty token ('') after the colon (':') token in \"ASSISTANT:\"\n        # in order for the predictions to match the training configuration and be accurate.\n        input_ids = torch.cat(\n            (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1\n        )\n\n        # Run VLA inference\n        generated_ids = self.generate(input_ids, max_new_tokens=self.get_action_dim(unnorm_key), **kwargs)\n\n        # Extract predicted action tokens and translate into (normalized) continuous actions\n        predicted_action_token_ids = generated_ids[0, -self.get_action_dim(unnorm_key) :].cpu().numpy()\n        discretized_actions = self.vocab_size - predicted_action_token_ids\n        discretized_actions = np.clip(discretized_actions - 1, a_min=0, a_max=self.bin_centers.shape[0] - 1)\n        normalized_actions = self.bin_centers[discretized_actions]\n\n        # Unnormalize actions\n        action_norm_stats = self.get_action_stats(unnorm_key)\n        mask = action_norm_stats.get(\"mask\", np.ones_like(action_norm_stats[\"q01\"], dtype=bool))\n        action_high, action_low = np.array(action_norm_stats[\"q99\"]), np.array(action_norm_stats[\"q01\"])\n        actions = np.where(\n            mask,\n            0.5 * (normalized_actions + 1) * (action_high - action_low) + action_low,\n            normalized_actions,\n        )\n\n        return actions\n"
  }
}